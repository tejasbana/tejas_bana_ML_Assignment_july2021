{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_tagging_pretrained_V2.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSmNOjXvTWrV"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCA_-rchD_Kb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as tt\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cH4c1oQ-ssa"
      },
      "source": [
        "# !pip install -U pandas-profiling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbmKCgAQOmVD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JtEFCC-AHgb"
      },
      "source": [
        "# !pip install -U featuretools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfo5hgaH52Dj"
      },
      "source": [
        "# !jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ3RclxUiKOY"
      },
      "source": [
        "# Exploring Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_ByP88DIx6Q"
      },
      "source": [
        "# Make copy of dataset to colab storage\n",
        "!cp /content/drive/MyDrive/Category\\ and\\ Attribute\\ Prediction\\ Benchmark/Img/img.zip ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H_cSkI8WgIk"
      },
      "source": [
        "!unzip ./img.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK_DSOi7WyQS"
      },
      "source": [
        "\"\"\"\n",
        "- Fashion Landmark Annotations (Anno/list_landmarks.txt)\n",
        "\tfashion landmark labels. See LANDMARK LABELS section below for more info.\n",
        "\n",
        "- Category Annotations (Anno/list_category_cloth.txt & Anno/list_category_img.txt)\n",
        "\tclothing category labels. See CATEGORY LABELS section below for more info.\n",
        "\n",
        "- Attribute Annotations (Anno/list_attr_cloth.txt & Anno/list_attr_img.txt)\n",
        "\tclothing attribute labels. See ATTRIBUTE LABELS section below for more info.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb_22sgy4gFm"
      },
      "source": [
        "# f = open(\"./list_category_img.txt\", \"r\")\n",
        "# print(f.readlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPcNqySPyL5i"
      },
      "source": [
        "!cp /content/drive/MyDrive/Category\\ and\\ Attribute\\ Prediction\\ Benchmark/Anno_coarse/list_category_img.txt ./\n",
        "!cp /content/drive/MyDrive/Category\\ and\\ Attribute\\ Prediction\\ Benchmark/Anno_coarse/list_attr_img.txt ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyX3oqFEz8Jr"
      },
      "source": [
        "import pandas as pd\n",
        "df_attr = pd.read_csv(\"./list_category_img.txt\", \n",
        "                       chunksize=10, \n",
        "                       sep = '\\s+',\n",
        "                       skiprows=[0,1],\n",
        "                       names = ['Image','Category'])\n",
        "\n",
        "result = df_attr.get_chunk(1000000)\n",
        "result = result[result.Image != \"img/Sheer_Pleated-Front_Blouse/img_00000001.jpg\"]\n",
        "print(result)\n",
        "\n",
        "np_att = np.array(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqHCwQWlC0Ps"
      },
      "source": [
        "np_att[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqqmyUh2_fgL"
      },
      "source": [
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "for i in range(np_att.shape[0]):\n",
        "    img_path = os.path.join('./', np_att[i+2][0])\n",
        "    png = Image.open(img_path)\n",
        "    category = np_att[i+2][1]\n",
        "    # print(png.load()) \n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us0WREPlDNp_"
      },
      "source": [
        "png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qiui7YY6koV"
      },
      "source": [
        "category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KcouJkeiF2y"
      },
      "source": [
        "# Custom Dataset Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKn2fVQUjGca"
      },
      "source": [
        "## Exploring attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGxSiCLidp6y"
      },
      "source": [
        "import pandas as pd\n",
        "df_attr = pd.read_csv(\"./list_attr_img.txt\", \n",
        "                       chunksize=10, \n",
        "                       sep = '\\s+',\n",
        "                       skiprows=[0,1],\n",
        "                    #    names = ['Image','at']\n",
        "                      )\n",
        "\n",
        "result = df_attr.get_chunk(1000000)\n",
        "print(result)\n",
        "\n",
        "np_att = np.array(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyhDaqpAmek-"
      },
      "source": [
        "np_att[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq2DbUac_CK7"
      },
      "source": [
        "len(np_att[0][1:])\n",
        "def one_hot_to_number(np_array):\n",
        "    # out = np.where(np_array == 1)\n",
        "    np_array[ np_array == -1] = 0\n",
        "    return list(np_array)\n",
        "\n",
        "len(one_hot_to_number(np_att[0][1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehhqbWRXdsOB"
      },
      "source": [
        "dataset_pairs = {}\n",
        "for idx in range(np_att.shape[0]):\n",
        "    dataset_pairs[np_att[idx][0]] = one_hot_to_number(np_att[idx][1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J3yRPd77mVf"
      },
      "source": [
        "# Custom dataset for zipping input and output images\n",
        "class CustomDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, combine_dir, transform, transform_size=32, data_len=1000000):\n",
        "        # One Folder with training inputs and outputs \n",
        "        self.combine_dir = combine_dir\n",
        "        self.both_dir = os.listdir(combine_dir)\n",
        "        self.transform = transform\n",
        "        self.input_dir = './'\n",
        "        self.dataset_pairs = dataset_pairs\n",
        "        all_imgs = []\n",
        "        category = []\n",
        "\n",
        "        # Category \n",
        "        df_category = pd.read_csv(\"./list_category_img.txt\", \n",
        "                       chunksize=10, \n",
        "                       sep = '\\s+',\n",
        "                       skiprows=[0,1],\n",
        "                       names = ['Image','Category'])\n",
        "        result = df_category.get_chunk(data_len)\n",
        "        result = result[result.Image != \"img/Sheer_Pleated-Front_Blouse/img_00000001.jpg\"]\n",
        "        self.np_cat = np.array(result)\n",
        "        # Deleting one extra element\n",
        "        # self.np_cat = np.delete(self.np_cat, 'img/Sheer_Pleated-Front_Blouse/img_00000001.jpg')\n",
        "\n",
        "        for i in range(self.np_cat.shape[0]):\n",
        "            all_imgs.append(os.path.join('./', self.np_cat[i][0]))\n",
        "            category.append(self.np_cat[i][1])\n",
        "        \n",
        "\n",
        "        # Attribute\n",
        "        # df_attr = pd.read_csv(\"./list_attr_img.txt\", \n",
        "        #                chunksize=10, \n",
        "        #                sep = '\\s+',\n",
        "        #                skiprows=[0,1],\n",
        "        #             #    names = ['Image','Category']\n",
        "        #               )\n",
        "\n",
        "        # result = df_attr.get_chunk(1000000)\n",
        "        # np_att = np.array(result)\n",
        "\n",
        "        \n",
        "        # for idx in range(np_att.shape[0]):\n",
        "        #     self.dataset_pairs[np_att[idx][0]] = one_hot_to_number(np_att[idx][1:])\n",
        "\n",
        "\n",
        "        self.total_imgs  = all_imgs #natsort.natsorted(all_imgs)\n",
        "        self.totla_cate  = category\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.total_imgs)\n",
        "\n",
        "    def one_hot_to_number(np_array):\n",
        "        # out = np.where(np_array == 1)\n",
        "        np_array[ np_array == -1] = 0\n",
        "        return list(np_array)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load input images\n",
        "        # if self.np_cat[idx][0] in self.dataset_pairs.keys():\n",
        "        # 'img/Sheer_Pleated-Front_Blouse/img_00000001.jpg'\n",
        "        in_img_loc = os.path.join(self.input_dir, self.total_imgs[idx])\n",
        "        input_image = Image.open(in_img_loc).convert(\"RGB\")\n",
        "        input_tensor = self.transform(input_image)\n",
        "\n",
        "        # Load output catogery\n",
        "        catogery_label = torch.tensor(self.totla_cate[idx])\n",
        "\n",
        "        # Load output Attribute of same image\n",
        "        att_label = torch.tensor(self.dataset_pairs[self.np_cat[idx][0]])\n",
        "\n",
        "        return input_tensor, catogery_label, att_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8klTqUWbNjWS"
      },
      "source": [
        "size = 256\n",
        "batch_size = 64\n",
        "dataset = CustomDataSet(\"./\", \n",
        "                        transform=Compose([ \n",
        "                        tt.Resize((size,size),interpolation=Image.ANTIALIAS),\n",
        "                        tt.ToTensor()]),\n",
        "                        transform_size=size,\n",
        "                        data_len=1000000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnSOnYjsfip6"
      },
      "source": [
        "len(dataset.np_cat[:,0]), len(dataset.dataset_pairs.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU-FMsUMOFNT"
      },
      "source": [
        "# Dowloading dataset and defining dataloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2,\n",
        "                                           pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STVJFR5AoQ56"
      },
      "source": [
        "# Validation DataLoader\n",
        "val_dataset = CustomDataSet(\"./\", \n",
        "                        transform=Compose([ \n",
        "                        tt.Resize((size,size),interpolation=Image.ANTIALIAS),\n",
        "                        tt.ToTensor()]),\n",
        "                        transform_size=size,\n",
        "                        data_len=10000)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                           batch_size=batch_size*4,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2,\n",
        "                                           pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl3ubugjOgpR"
      },
      "source": [
        "# Define function to display dataset\n",
        "def denorm(img_tensor):\n",
        "    return img_tensor\n",
        "\n",
        "def show_images(images):\n",
        "    fig, ax = plt.subplots(figsize=(12,12))\n",
        "    ax.set_xticks([]); ax.set_yticks([]);\n",
        "    print(images.shape)\n",
        "    ax.imshow( make_grid( denorm(images.detach()[:64]), nrow=8).permute((1,2,0)))\n",
        "    \n",
        "\n",
        "def show_batch(data_loader):\n",
        "    for images, category, attribute in data_loader:\n",
        "        show_images(images)\n",
        "        return category, attribute\n",
        "\n",
        "def show_output(data_loader):\n",
        "    for _, images in data_loader:\n",
        "        show_images(images)\n",
        "        break;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la3JTCCmOvP-"
      },
      "source": [
        "# Display training samples\n",
        "category, attribute = show_batch(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpOyX9ZnOxT2"
      },
      "source": [
        "category.shape, attribute.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C625JTwuPANK"
      },
      "source": [
        "# Setup GPU configuration and DeviceDataLoader for efficient GPU memory usage\n",
        "def get_default_device():\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def to_device(data, device):\n",
        "    ''' Loading data to device '''\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in  data]\n",
        "    else:\n",
        "        return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for batch in self.dl:\n",
        "            yield to_device(batch, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvpkqB7Wh82G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PWCFbA-h9X_"
      },
      "source": [
        "# Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ_bJRuRQpeZ"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from vit_pytorch import ViT\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "def accuracy2(outputs, labels):\n",
        "    # gt_S  =np.asarray(gt_S)\n",
        "    outputs = F.sigmoid(outputs) \n",
        "    # print(outputs.shape)\n",
        "    outputs = torch.round(outputs)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "    # return torch.tensor(torch.sum(outputs*labels).item() / int(torch.sum(labels*labels).item()))\n",
        "\n",
        "\n",
        "class Base(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        input, category, attribute = batch\n",
        "        out_category, out_attribute = self(input)                  # Generate predictions\n",
        "        attribute = attribute.type_as(out_attribute)\n",
        "        loss1 = 100*F.cross_entropy(out_category, category)  # Calculate loss\n",
        "        loss2 = F.binary_cross_entropy_with_logits(out_attribute, attribute)\n",
        "        return loss1 + loss2\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        input, category, attribute = batch\n",
        "        out_category, out_attribute = self(input)                    # Generate predictions\n",
        "        attribute = attribute.type_as(out_attribute)\n",
        "        loss1 = F.cross_entropy(out_category, category)  # Calculate loss\n",
        "        loss2 = 100*F.binary_cross_entropy_with_logits(out_attribute, attribute)\n",
        "        acc = accuracy(out_category, category)           # Calculate accuracy\n",
        "        acc2 = accuracy2(out_attribute, attribute)           # Calculate accuracy\n",
        "        return {'val_loss': loss1.detach() + loss2.detach(), 'val_acc': acc, 'val_acc2': acc2}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        # Attribute acc\n",
        "        batch_accs2 = [x['val_acc2'] for x in outputs]\n",
        "        epoch_acc2 = torch.stack(batch_accs2).mean()\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(), 'val_acc2': epoch_acc2.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}],{} train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, val_acc2: {:.4f}\".format(\n",
        "            epoch, \"last_lr: {:.5f},\".format(result['lrs'][-1]) if 'lrs' in result else '', \n",
        "            result['train_loss'], result['val_loss'], result['val_acc'], result['val_acc2']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9gX4SWjbpdK"
      },
      "source": [
        "class Model(Base):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = torchvision.models.resnext50_32x4d(pretrained=True) #torchvision.models.wide_resnet101_2(pretrained=True)\n",
        "        \n",
        "        self.model.fc = nn.Sequential( nn.Linear(2048, 2048),\n",
        "                                       nn.ReLU(inplace=True))\n",
        "\n",
        "        self.category_head = nn.Sequential( nn.Linear(2048, 50))\n",
        "        self.attribute_head = nn.Sequential( nn.Linear(2048, 1000))\n",
        "\n",
        "    def forward(self, images):\n",
        "        out = self.model(images)\n",
        "        pred_category = self.category_head(out)\n",
        "        pred_attribute = self.attribute_head(out)\n",
        "\n",
        "        return pred_category, pred_attribute\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTT7viKedYMH"
      },
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, optimizer,opt_func=torch.optim.SGD):\n",
        "    history = []\n",
        "    # optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "\n",
        "    # Set up custom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            if grad_clip:\n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0DSES1FSJ9f"
      },
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ5tUeBJSSya"
      },
      "source": [
        "train_dl = DeviceDataLoader(train_loader, device)\n",
        "valid_dl = DeviceDataLoader(val_loader, device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNbbVRvbSWfJ"
      },
      "source": [
        "model = Model()\n",
        "to_device(model, device)\n",
        "\n",
        "# Evaluate\n",
        "# evaluate(model, valid_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS0hmBbRSe22"
      },
      "source": [
        "epochs = 6\n",
        "max_lr = 0.01\n",
        "grad_clip = None#0.1\n",
        "weight_decay = 0#1e-4\n",
        "opt_func = torch.optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdTUN2ZrShDf"
      },
      "source": [
        "# %%time\n",
        "history = fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
        "                         grad_clip=grad_clip, \n",
        "                         weight_decay=weight_decay, \n",
        "                         opt_func=opt_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpZY9Ol32NO5"
      },
      "source": [
        "lr = 0.0015\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwlbqAN12WoV"
      },
      "source": [
        "history += fit(epochs, lr, model, train_dl, valid_dl , optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN3-caBSZoSH"
      },
      "source": [
        "lr = 0.0002\n",
        "history += fit(epochs, lr, model, train_dl, valid_dl , optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDoADSEppZls"
      },
      "source": [
        "states = {\n",
        "            'epoch': epochs + 1,\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "torch.save(states, \"./resnext_6.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJbTjsjm2B8B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}